{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ammar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ammar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Ammar\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have to say, Apple has by far the best custo...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iOS 7 is so fricking smooth &amp; beautiful!! #Tha...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOVE U @APPLE</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thank you @apple, loving my new iPhone 5S!!!!!...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.@apple has the best customer service. In and ...</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  Avg\n",
       "0  I have to say, Apple has by far the best custo...  2.0\n",
       "1  iOS 7 is so fricking smooth & beautiful!! #Tha...  2.0\n",
       "2                                      LOVE U @APPLE  1.8\n",
       "3  Thank you @apple, loving my new iPhone 5S!!!!!...  1.8\n",
       "4  .@apple has the best customer service. In and ...  1.8"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'./tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1181,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df['Tweet']\n",
    "df = df.astype('str')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I have to say, Apple has by far the best custo...\n",
       "1    iOS 7 is so fricking smooth & beautiful!! #Tha...\n",
       "2                                        LOVE U @APPLE\n",
       "3    Thank you @apple, loving my new iPhone 5S!!!!!...\n",
       "4    .@apple has the best customer service. In and ...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Removing Punctuation Symbols and Stop Words From Tweets\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding custom stopwords\n",
    "new_stop_words = [\n",
    "    'some', 'like', 'think', 'wow', 'one', 'http', 'web', 'really', \n",
    "    'see', 'watch', 'apple', 'know', 'show', 'think', 'click', 'go', 'to', 'great', \n",
    "    'very', 'good', 'many', 'more', 'people', 'made', 'technology', 'tech',\n",
    "    'iphone', 'ipad', 'new', 'latest', 'phone', 'itunes', 'brand', 'ipod', 'iphones', \n",
    "    'io', 'get', 'buy', 'purchase', 'make', 'im', \"iam\", 'dont', 'cant', 'promoipodplayerpromo',\n",
    "    'ipodplayerpromo', 'player', 'itune']\n",
    "stop_words = stop_words.union(new_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'teaser trailer macbook pro freak'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Pre-Processing\n",
    "corpus = []\n",
    "for i, line in df.iteritems():\n",
    "    line = line.lower()\n",
    "    line = re.sub(r\"\\d+\", \"\", line)\n",
    "    line = line.translate(translate_table)\n",
    "    line = line.split()\n",
    "    \n",
    "    # Lemmatizers reduces each word to its root/canonical form\n",
    "    lm = WordNetLemmatizer()\n",
    "    line = [lm.lemmatize(word) for word in line if not word in stop_words]\n",
    "    line = \" \".join(line)\n",
    "    corpus.append(line)\n",
    "\n",
    "corpus[1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We ignore the words that appear in 70% of documents in the corpus\n",
    "def get_top_keywords(corpus, upto=None):    \n",
    "    cv = CountVectorizer(max_df=0.7,\n",
    "                        stop_words=stop_words,\n",
    "                        ngram_range=(2,3),\n",
    "                        min_df=0.001)\n",
    "    X = cv.fit_transform(corpus)\n",
    "    bag_of_words = cv.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x : x[1], reverse=True)\n",
    "    return words_freq[:upto]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = get_top_keywords(corpus, upto=70)\n",
    "top_words_df = pd.DataFrame(top_words)\n",
    "top_words_df.columns = ['Word', 'Frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>facebook amazon</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>card app</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fingerprint scanner</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>steve job</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vp marketing</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>marketing quiet</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>quiet photogs</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>photogs let</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>let photography</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>condescension anyone</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Word  Frequency\n",
       "0       facebook amazon         11\n",
       "1              card app          9\n",
       "2   fingerprint scanner          8\n",
       "3             steve job          7\n",
       "4          vp marketing          7\n",
       "5       marketing quiet          7\n",
       "6         quiet photogs          7\n",
       "7           photogs let          7\n",
       "8       let photography          7\n",
       "9  condescension anyone          7"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to extract keywords (Using TF-IDF Vectorizer)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf = TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 2), min_df=0.001)\n",
    "tf_vocab = tf_idf.fit_transform(corpus)\n",
    "tf_vocab = tf_idf.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aapl',\n",
       " 'absolutely',\n",
       " 'abt',\n",
       " 'access',\n",
       " 'accidentally',\n",
       " 'acciones',\n",
       " 'acciones de',\n",
       " 'account',\n",
       " 'acting',\n",
       " 'activist']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = tf_idf.get_feature_names()\n",
    "feature_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('promo', 18.90847376192999),\n",
       " ('freak', 17.17697860023629),\n",
       " ('store', 14.155237274781651),\n",
       " ('iphonec', 14.09608028645135),\n",
       " ('need', 12.001938748005974),\n",
       " ('time', 10.586806086811908),\n",
       " ('fingerprint', 10.454660578368726),\n",
       " ('thanks', 10.349970563467519),\n",
       " ('want', 9.713939612056656),\n",
       " ('de', 9.687285268615476),\n",
       " ('app', 9.422630904822924),\n",
       " ('would', 9.234892026321225),\n",
       " ('rt', 9.234569335351923),\n",
       " ('come', 9.08323098622442),\n",
       " ('hate', 9.027584016005337),\n",
       " ('look', 8.71127368495836),\n",
       " ('lol', 8.395419577608036),\n",
       " ('love', 8.290610155691734),\n",
       " ('better', 8.1330285471406),\n",
       " ('samsung', 7.994020693427633),\n",
       " ('generation', 7.812109373221487),\n",
       " ('android', 7.691626364749863),\n",
       " ('google', 7.644589459067082),\n",
       " ('please', 7.438880601565536),\n",
       " ('twitter', 7.349086435926864),\n",
       " ('battery', 7.256338447322632),\n",
       " ('guy', 7.237185640858383),\n",
       " ('microsoft', 7.154458874644534),\n",
       " ('give', 7.0109480231317),\n",
       " ('back', 7.008423730942729)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_words = tf_vocab.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx])\n",
    "              for word, idx in tf_idf.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "words_freq[:30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "7c2737f5cd3eb6a237b7123ce75c641d6f975db18b0c0702ad2055474d78171c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
